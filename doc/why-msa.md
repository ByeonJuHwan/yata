# MSA 전환의 필요성

## 현재 시스템 구조

현재 `hotel-management-service`는 다음과 같은 여러 도메인과 책임을 가지고 있습니다:

**조회(Read) 기능**
- 호텔 목록 조회 (사용자)
- 호텔 상세 정보 조회 (사용자)

**등록/수정(Write) 기능**
- 호텔 및 객실 등록/수정 (사업자)
- **예약 생성 및 관리 (사용자)**

### 트래픽 특성

| 기능 | 특성 | 비율 |
|------|------|------|
| 조회 | Read-Heavy, 캐싱 가능 | 90% |
| 예약 | Write-Heavy, 트랜잭션 필수 | 10% |


트래픽 특성이 완전히 다른 기능들이 같은 리소스(Thread Pool, DB Connection)를 공유

이처럼 여러 도메인(카탈로그, 예약)이 혼재되어 있고, Read/Write 작업이 
하나의 서비스에서 처리되는 모놀리식 구조입니다.
---
## 2. 주요 문제점

### 문제 1: 장애 전파 (Failure Propagation)

**시나리오:**
```
예약 트래픽 급증 (50명 → 동일 객실 예약)
    ↓
DB Lock 대기 시간 증가 (평균 1.6초)
    ↓
Thread Pool & DB Connection 점유
    ↓
조회 요청도 같은 리소스 대기
    ↓
조회 성능 저하 (0.5초 → 3.3초)
```

**핵심 문제:**
> 예약과 무관한 조회 사용자도 함께 피해를 받음

### 문제 2: 리소스 경합

**공유 리소스:**
- Tomcat Thread Pool: 200개 (공통)
- DB Connection Pool: 10개 (공통)

**경합 발생:**
```
예약 50명 처리 중
├─> Thread Pool: 80개 점유 (40%)
├─> DB Connection: 8개 점유 (80%)
└─> 조회 요청이 남은 리소스로 처리 ⚠️
    └─> 부족하면 대기 큐에서 대기
        └─> 응답 시간 증가
```

### 문제 3: 독립적 확장 불가능

**현재 문제:**
```
조회 트래픽 10배 증가
    ↓
서버 10대로 스케일 아웃
    ↓
문제: 예약용 DB Connection도 10배 증가 (낭비)
      예약 로직도 10배 복제 (불필요)
```

![단일서비스 확장](https://velog.velcdn.com/images/chapchap/post/4727b267-df40-4879-8db2-8c67e84104d8/image.png)


**필요한 것:**
- 조회만 수평 확장 (캐싱 + Read Replica)
- 예약은 현재 용량 유지

---

## 3. 부하 테스트를 통한 문제 검증

### 3.1 테스트 환경

#### 아키텍처
![단일 서비스 부하테스트](https://velog.velcdn.com/images/chapchap/post/ced2a5bd-fe5e-4190-9680-5612ae3dbae6/image.png)

#### 서버 스펙
- 플랫폼: 네이버 클라우드 플랫폼
- 서버 타입: Standard (s2-g3a)
- 스펙: vCPU 2EA, Memory 8GB
- OS: Ubuntu 22.04 LTS

#### 애플리케이션 설정
```yaml
# Spring Boot 설정
server.tomcat.threads.max: 200
spring.datasource.hikari.maximum-pool-size: 10
```

#### 테스트 도구
- k6 (부하 테스트)
- 로컬 MacBook → NCP 서버

---

### 3.2 테스트 시나리오

#### 부하 조건
| 구분 | 설정 | 목적 |
|------|------|------|
| **조회** | 50 VU (일정) | 정상 트래픽 유지 |
| **예약** | 5 → 50 VU (10배 증가) | 부하 증가 시뮬레이션 |
| **예약 대상** | 동일 객실 1개 | DB Lock 경합 유도 |
| **기간** | 4분 | 단계별 관찰 |

#### 예약 프로세스 (현실적 시나리오)
```kotlin
@Transactional
fun createReservation() {
    // 1. 결제 API 호출 (외부 PG사)
    processPayment()  // 평균 1초 소요
    
    // 2. DB Lock 획득 및 재고 확인
    val room = roomRepository.getRoomStockWithLock(roomId)
    
    // 3. 재고 차감
    room.deductStock()
    
    // 4. 예약 생성
    reservationRepository.save(reservation)
    
    // Lock 해제 (전체 소요: 약 2초)
}
```

**문제점:**
- 결제 API 대기 중에도 **DB Lock 보유** (또는 Thread 점유)
- 50명이 순차 대기 → 누적 대기 시간 발생
- 조회 요청도 같은 Thread Pool 사용 → 대기 발생

---

### 3.3 테스트 결과

#### 전체 결과
| 서비스 | 지표 | 목표 | 실제 | 평가 |
|--------|------|------|------|------|
| **예약** | P95 응답시간 | <3,000ms | 2,999ms | ✅ 정상 |
| **예약** | 성공률 | >95% | 100% | ✅ 완벽 |
| **조회** | P95 응답시간 | <500ms | **3,323ms** | ❌ **6.6배 초과** |
| **조회** | P99 응답시간 | <1,000ms | **9,604ms** | ❌ **9.6배 초과** |
| **조회** | 성공률 | >99% | 99.06% | ⚠️ 일부 실패 |

#### 상세 분석

**예약 성능 (정상):**
```
평균 응답시간: 1,628ms
P95: 2,999ms ✅
P99: 3,763ms ✅
성공률: 100% (1,626/1,626)

→ 예약 자체는 정상 처리됨
```

**조회 성능 (심각한 저하):**
```
평균 응답시간: 743ms (정상: <100ms)
P95: 3,323ms ❌ (목표: 500ms)
P99: 9,604ms ❌ (목표: 1,000ms)
성공률: 99.06% (65건 타임아웃)

→ 조회 요청의 5%는 3초 이상 대기
→ 1%는 거의 10초 대기 (타임아웃 직전)
```

### 3.4 핵심 문제점

####  예약은 정상, 조회만 느려짐!
```
┌─────────────────────────────────────┐
│         예약 50명 처리 중              │
│         (모두 같은 객실)               │
└─────────────────────────────────────┘
              ▼
┌─────────────────────────────────────┐
│   Thread Pool & Connection 점유      │
│   • Thread: ~80개 사용                │
│   • DB Connection: ~8개 사용          │
└─────────────────────────────────────┘
              ▼
┌─────────────────────────────────────┐
│   조회 요청 50명 들어옴                 │
│   • 남은 Thread: 120개               │
│   • 남은 Connection: 2개             │
└─────────────────────────────────────┘
              ▼
┌─────────────────────────────────────┐
│        대기 큐에서 대기                 │
│   • 평균 대기: 0.7초                   │
│   • P95: 3.3초                       │
│   • P99: 9.6초                       │
└─────────────────────────────────────┘
```

**결론:**
> 예약 부하 증가 시 조회 성능이 **6.6배 저하**  
> 예약과 무관한 사용자도 **평균 7배 느린** 응답 경험

---

# MSA 전환

부하 테스트 결과, 예약 부하 증가 시 조회 성능이 6.6배 저하되는 것이 가장 큰 문제로 확인되었습니다. 

예약 트래픽이 급증하면 DB Lock과 Thread Pool 경합으로 인해 예약과 무관한 조회 사용자까지 평균 3초 이상 대기해야 하는 상황이 발생했습니다. 

따라서 hotel-management-service를 두 개의 독립적인 서비스로 분리했습니다. 

첫 번째는 hotel-service로, 사용자들의 호텔 조회와 검색을 전담하는 Read 중심의 서비스입니다. 두 번째는 reservation-service로, 예약 생성 및 재고 관리를 담당하는 Write 중심의 서비스입니다. 

이렇게 분리함으로써 각 서비스는 독립적인 Thread Pool, DB Connection Pool, 그리고 서버 인스턴스를 가지게 되어 리소스 경합 문제가 근본적으로 해결됩니다.

이러한 분리의 가장 큰 장점은 독립적인 확장이 가능하다는 점입니다. 

조회 요청이 폭주할 경우 hotel-service만 스케일 아웃하여 대응할 수 있으며, 이때 Redis 캐싱과 Read Replica를 활용해 DB 부하를 최소화할 수 있습니다. 

반대로 예약 요청이 급증하면 reservation-service만 독립적으로 확장하되, 트랜잭션 안정성에 초점을 맞춘 수직 확장 또는 DB Connection Pool 조정으로 대응할 수 있습니다. 더 중요한 것은 장애 격리입니다. 

예약 서비스에 장애가 발생하더라도 조회 서비스는 정상적으로 동작하여 고객들이 여전히 호텔을 검색하고 비교할 수 있습니다. 이는 전체 서비스 다운을 방지하고 비즈니스 연속성을 보장하는 핵심 요소입니다.

또한 MSA 전환은 기술적 이점뿐만 아니라 비용 효율성도 가져옵니다. 기존에는 조회 트래픽 증가로 서버를 10대로 늘리면 불필요한 예약용 리소스까지 10배 복제되어 비용 낭비가 발생했습니다. 하지만 분리 후에는 조회 서비스 10대, 예약 서비스 3대처럼 트래픽 특성에 맞춰 최적화된 스케일링이 가능해져 서버 비용을 약 40% 절감할 수 있습니다.

이로써 Yata 프로젝트는 대규모 트래픽에 유연하게 대응하기 위해 MSA를 도입하여 서비스 안정성과 확장성을 목표로 개발해 보겠습니다.